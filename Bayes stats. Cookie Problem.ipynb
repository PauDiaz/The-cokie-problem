{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian stats\n",
    "\n",
    "All the content of this notebook has been mostly extracted from the book \"Think Bayes\" \n",
    "Bayesian Statistics Made Simple by **Allen B. Downey**\n",
    "\n",
    "## Conjoint probability\n",
    "\n",
    "**Conjoint probability** is a fancy way to say the probability that two things are true. I write **p(A and B)** to mean the probability that A and B are both true.\n",
    "If you learned about probability in the context of coin tosses and dice, you might have learned the following formula:\n",
    "\n",
    "    p(A and B) = p(A) p(B)           WARNING: not always true\n",
    "\n",
    "For example, if I toss two coins, and A means the first coin lands face up, and B means the second coin lands face up, then p(A) = p(B) = 0.5, and sure enough, p(A and B) = p(A) p(B) = 0.25.\n",
    "\n",
    "But this formula only works because in this case **A and B are independent**.\n",
    "\n",
    "In general, the probability of a conjunction is\n",
    "    \n",
    "    p(A and B) = p(A) p(B|A)\n",
    "    \n",
    "## The cokie problem\n",
    "\n",
    "We’ll get to Bayes’s theorem soon, but I want to motivate it with an example called the cookie problem.\n",
    "\n",
    "Suppose there are two bowls of cookies. \n",
    "* Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies. \n",
    "* Bowl 2 contains 20 of each.\n",
    "\n",
    "Now suppose you choose one of the bowls at random and, without looking, select a cookie at random. The cookie is vanilla. What is the probability that it came from Bowl 1?\n",
    "\n",
    "\n",
    "This is a conditional probability; we want p(Bowl 1|vanilla), but it is not obvious how to compute it. If I asked a different question—the probability of a vanilla cookie given Bowl 1—it would be easy:\n",
    "\n",
    "    p(vanilla|Bowl 1) = 3/4\n",
    "    \n",
    "Sadly, p(A|B) is not the same as p(B|A), but there is a way to get from one\n",
    "to the other: Bayes’s theorem.\n",
    "\n",
    "**Bayes’s theorem**\n",
    "\n",
    "At this point we have everything we need to derive Bayes’s theorem. We’ll start with the observation that conjunction is commutative; that is\n",
    "\n",
    "    p(A and B) = p(B and A) for any events A and B.\n",
    "\n",
    "Next, we write the probability of a conjunction:\n",
    "\n",
    "    p(A and B) = p(A) p(B|A)\n",
    "\n",
    "Since we have not said anything about what A and B mean, they are interchangeable. Interchanging them yields:\n",
    "\n",
    "    p(B and A) = p(B) p(A|B)\n",
    "\n",
    "\n",
    "That’s all we need. Pulling those pieces together, we get \n",
    "\n",
    "    p(B) p(A|B) = p(A) p(B|A)\n",
    "    \n",
    "Which means there are two ways to compute the conjunction. If you have p(A), you multiply by the conditional probability p(B|A). Or you can do it the other way around; if you know p(B), you multiply by p(A|B). Either way you should get the same thing.\n",
    "Finally we can divide through by p(B):\n",
    "\n",
    "    p(A|B) = p(A) p(B|A) / p(B)\n",
    "\n",
    "And that’s **Bayes’s theorem!** It might not look like much, but it turns out to\n",
    "be surprisingly powerful.\n",
    "For example, we can use it to solve the cookie problem. I’ll write B1 for the hypothesis that the cookie came from Bowl 1 and V for the vanilla cookie. Plugging in Bayes’s theorem we get\n",
    "\n",
    "    p(B1|V) = p(B1) p(V|B1) / p(V)\n",
    "    \n",
    "* p(B1|V) probability of being Bowl 1 given that the cookie is vainilla (posterior)\n",
    "* p(B1) prob of picking bowl 1 (0.5)\n",
    "* p(V|B1) prob of taking a vanilla cookie from bowl 1 (30/40 = 3/4) (Prior)\n",
    "* p(V) prob of taking any vanilla cookie ((30+20)/80 = 5/8)\n",
    "\n",
    "\n",
    "    p(B1|V) = (0.5)*(3/4)/(5/8) = 3/5 = 0.6\n",
    "\n",
    "In the context of Bayes’s theorem, it is natural to use a Pmf to map from each hypothesis to its probability. In the cookie problem, the hypotheses are B1 and B2. In Python, I represent them with strings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from thinkbayes import Pmf\n",
    "pmf = Pmf()\n",
    "pmf.Set('Bowl 1',0.5)\n",
    "pmf.Set('Bowl 2',0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution, which contains the priors for each hypothesis, is called the **prior distribution**. \n",
    "\n",
    "To update the distribution based on new data (the vanilla cookie), we mul- tiply each prior by the corresponding likelihood. The likelihood of drawing a vanilla cookie from Bowl 1 is 3/4. The likelihood for Bowl 2 is 1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmf.Mult('Bowl 1', 0.75) # prob de sacar una vanilla cokie del bowl1 es 30/40 = 3/4\n",
    "pmf.Mult('Bowl 2', 0.5) # prob de sacar una vanilla cokie del bowl2 es 20/40 = 2/4 = 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mult does what you would expect. It gets the probability for the given hy- pothesis and multiplies by the given likelihood.\n",
    "After this update, the distribution is no longer normalized, but because these hypotheses are mutually exclusive and collectively exhaustive, we can renormalize:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmf.Normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a distribution that contains the posterior probability for each hypothesis, which is called (wait now) the posterior distribution.\n",
    "Finally, we can get the posterior probability for Bowl 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "print pmf.Prob('Bowl 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the answer is 0.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
